{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from threading import Timer\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_park_urls = os.path.join('auxiliary', 'parks.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generally useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_start_times(freq = '5min'):\n",
    "\n",
    "    now = datetime.today()\n",
    "    start_day = now.replace(day = now.day + 1, hour = 0, minute = 0, second = 0, microsecond = 0)\n",
    "    days = pd.date_range(start_day, periods = 365, freq = '1D')\n",
    "\n",
    "    start_times = list()\n",
    "    freq = '5min'\n",
    "    for day in days:\n",
    "        log_times = pd.date_range(\n",
    "            start = day.replace(hour = 9),\n",
    "            end = day.replace(hour = 22),\n",
    "            freq = freq\n",
    "        )\n",
    "        start_times.extend(log_times)\n",
    "    return start_times\n",
    "\n",
    "def scrape_wait_times(start_time, park, url):\n",
    "    print(f\"Scraping {park} at {start_time}\")\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    #table_waiting = soup.find(id = 'container')\\\n",
    "    #                    .find(id = 'content-wrapper')\\\n",
    "    #                    .find(id = 'content')\\\n",
    "    #                    .find(id = 'rides')\n",
    "    table_waiting = soup.find(lambda tag: tag.name=='table' and tag.has_attr('id') and tag['id']==\"rides\")\n",
    "    #rows_waiting_rows = table_waiting.findAll(lambda tag: tag.name=='tr')\n",
    "\n",
    "    rows = list()\n",
    "    states = list()\n",
    "    for i in range(1, 10):\n",
    "        if not states:\n",
    "            states = table_waiting.find_all('td', class_ = f'state state_{i}')\n",
    "            if states:\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Found no valid table for state state_i for {park}\")\n",
    "\n",
    "    for name_html, waittime_html, state_html in zip(table_waiting.find_all('td', class_ = 'name'), \n",
    "                                                table_waiting.find_all('td', class_ = 'waittime'),\n",
    "                                                states):\n",
    "        name = name_html.getText()\n",
    "        state = state_html.getText()\n",
    "\n",
    "        waittime = re.sub(\"[^0-9]\", \"\", waittime_html.getText())\n",
    "        if waittime == \"\":\n",
    "            waittime = 0\n",
    "        else:\n",
    "            waittime = int(waittime)\n",
    "\n",
    "        rows.append({\n",
    "            \"name\" : name,\n",
    "            \"waittime (min)\" : waittime,\n",
    "            \"state\" : state\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    foldername = os.path.join(\"data\", park)\n",
    "    filename = start_time.strftime(f\"%Y%m%d_%H%M_{park}.csv\")\n",
    "    if not os.path.exists(foldername):\n",
    "        os.makedirs(foldername)\n",
    "    df.to_csv(os.path.join(foldername, filename))\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Paultons Park at 2023-03-12 09:00:00Scraping Walibi Belgium at 2023-03-12 09:00:00Scraping Bellewaerde at 2023-03-12 09:05:00Scraping Gardaland at 2023-03-12 09:05:00\n",
      "Scraping Futuroscope at 2023-03-12 09:05:00Scraping Chessington at 2023-03-12 09:10:00Scraping Thorpe Park at 2023-03-12 09:05:00Scraping Walt Disney Studios Park at 2023-03-12 09:15:00Scraping Energylandia at 2023-03-12 09:00:00\n",
      "Scraping Parque Warner at 2023-03-12 09:25:00Scraping Efteling at 2023-03-12 09:05:00\n",
      "Scraping Phantasialand at 2023-03-12 09:10:00Scraping Bobbejaanland at 2023-03-12 09:50:00\n",
      "\n",
      "\n",
      "\n",
      "Scraping FamilyPark at 2023-03-12 09:25:00Scraping Thorpe Park at 2023-03-12 09:55:00\n",
      "\n",
      "Scraping Tivoli Gardens at 2023-03-12 09:50:00Scraping Liseberg at 2023-03-12 10:10:00Scraping Plopsaland at 2023-03-12 09:05:00\n",
      "Scraping Walibi Holland at 2023-03-12 09:55:00\n",
      "Scraping Walibi Holland at 2023-03-12 10:35:00\n",
      "Scraping Paultons Park at 2023-03-12 10:45:00Scraping Plopsaland at 2023-03-12 09:00:00Scraping Alton Towers at 2023-03-12 09:40:00Scraping Disneyland Park at 2023-03-12 09:15:00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scraping Alton Towers at 2023-03-12 10:15:00Scraping Walt Disney Studios Park at 2023-03-12 09:00:00Scraping Thorpe Park at 2023-03-12 09:20:00Scraping Walibi Holland at 2023-03-12 09:00:00Scraping Parc Ast√©rix at 2023-03-12 11:00:00Scraping PortAventura at 2023-03-12 10:30:00Scraping Legoland Windsor at 2023-03-12 09:35:00Scraping Bobbejaanland at 2023-03-12 10:05:00Scraping Legoland Billund Resort at 2023-03-12 09:10:00Scraping Legoland Windsor at 2023-03-12 10:40:00Scraping PortAventura at 2023-03-12 10:40:00\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read all urls\n",
    "with open(loc_park_urls, \"r\") as park_url_file:\n",
    "    park_urls = json.load(park_url_file)\n",
    "\n",
    "# Select start times\n",
    "start_times = create_start_times(freq = '5min')\n",
    "\n",
    "for start_time in start_times:\n",
    "    num_seconds = (start_time - datetime.now()).seconds\n",
    "    for park, url in park_urls.items():\n",
    "        t = Timer(10, scrape_wait_times, [start_time, park, url])\n",
    "        t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
